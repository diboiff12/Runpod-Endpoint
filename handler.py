import runpod
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

print("=" * 50)
print("ğŸš€ Mistral 7B Handler ì‹œì‘")
print("=" * 50)

# ============================================
# ëª¨ë¸ ë¡œë“œ (ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œ 1ë²ˆë§Œ ì‹¤í–‰)
# ============================================

MODEL_ID = "mistralai/Mistral-7B-Instruct-v0.2"

print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_ID}")

# Tokenizer ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

# ëª¨ë¸ ë¡œë“œ (4-bit ì–‘ìí™”)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    load_in_4bit=True,           # 4-bit ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)
    device_map="auto",           # GPU ìë™ í• ë‹¹
    torch_dtype=torch.float16,   # FP16 ì‚¬ìš©
    trust_remote_code=True       # ì½”ë“œ ì‹¤í–‰ í—ˆìš©
)

print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
print(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")


# ============================================
# Handler í•¨ìˆ˜ (ê° ìš”ì²­ë§ˆë‹¤ ì‹¤í–‰)
# ============================================

def handler(job):
    """
    RunPodì´ API ìš”ì²­ë§ˆë‹¤ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        job (dict): {
            "input": {
                "prompt": str,
                "max_tokens": int (optional),
                "temperature": float (optional)
            }
        }
    
    Returns:
        dict: {
            "output": str,
            "tokens_generated": int
        }
    """
    
    try:
        # ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
        job_input = job.get("input", {})
        
        # í•„ìˆ˜ íŒŒë¼ë¯¸í„° í™•ì¸
        if "prompt" not in job_input:
            return {"error": "Missing required field: 'prompt'"}
        
        prompt = job_input["prompt"]
        max_tokens = job_input.get("max_tokens", 200)
        temperature = job_input.get("temperature", 0.7)
        
        print(f"\n{'='*50}")
        print(f"ğŸ“¨ ìš”ì²­ ìˆ˜ì‹ :")
        print(f"   Prompt: {prompt[:100]}...")
        print(f"   Max tokens: {max_tokens}")
        print(f"   Temperature: {temperature}")
        print(f"{'='*50}\n")
        
        # Mistral Instruct í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_prompt = f"<s>[INST] {prompt} [/INST]"
        
        # Tokenize
        inputs = tokenizer(formatted_prompt, return_tensors="pt").to("cuda")
        
        # ì¶”ë¡ 
        print("ğŸ¤– ì¶”ë¡  ì‹œì‘...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # [INST] ë¶€ë¶„ ì œê±° (ì‘ë‹µë§Œ ì¶”ì¶œ)
        if "[/INST]" in result:
            result = result.split("[/INST]")[-1].strip()
        
        tokens_generated = len(outputs[0])
        
        print(f"âœ… ì¶”ë¡  ì™„ë£Œ!")
        print(f"   ìƒì„±ëœ í† í°: {tokens_generated}")
        print(f"   ì‘ë‹µ ê¸¸ì´: {len(result)} ê¸€ì\n")
        
        return {
            "output": result,
            "tokens_generated": tokens_generated,
            "model": MODEL_ID
        }
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            "error": str(e),
            "type": type(e).__name__
        }


# ============================================
# RunPod Serverless ì‹œì‘
# ============================================

if __name__ == "__main__":
    print("\nğŸ¯ RunPod Serverless ëŒ€ê¸° ì¤‘...")
    print("API ìš”ì²­ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.\n")
    
    runpod.serverless.start({"handler": handler})